{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hlda.sampler import HierarchicalLDA\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"matrix_data.csv\",header=None)\n",
    "pd_vocab = pd.read_csv(\"words_dict.csv\",header=None)\n",
    "idx = data.sum()==1\n",
    "validcol = [col for i,col in enumerate(data.columns) if not idx[i] ]\n",
    "validwords = [word for i,word in enumerate(list(pd_vocab[0])) if not idx[i] ]\n",
    "reduced_data = data[validcol]\n",
    "reduced_data.columns = range(reduced_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "#index is the row index, row is the document \n",
    "for index, row in reduced_data.iterrows():\n",
    "    new_doc = []\n",
    "    for i in range(len(validwords)):\n",
    "        if row[i]!= 0:\n",
    "            for j in range(row[i]):\n",
    "                new_doc.append(i)\n",
    "    new_corpus.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 500       # no of iterations for the sampler\n",
    "alpha = 10.0          # smoothing over level distributions\n",
    "gamma = 1.0           # CRP smoothing parameter; number of imaginary customers at next, as yet unused table\n",
    "eta = 0.1             # smoothing over topic-word distributions\n",
    "num_levels = 2        # the number of levels in the tree\n",
    "display_topics = 50   # the number of iterations between printing a brief summary of the topics so far\n",
    "n_words = 10          # the number of most probable words to print for each topic after model estimation\n",
    "with_weights = False  # whether to print the words with the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HierarchicalLDA sampling\n",
      ".................................................. 50\n",
      "topic 0 (level=0, total_words=71757, documents=14640): flight, not, thank, get, no, can, just, will, now, time, \n",
      "    topic 1 (level=1, total_words=3519, documents=636): hour, delay, plane, flight, gate, sit, wait, late, connect, min, \n",
      "    topic 2 (level=1, total_words=4004, documents=728): hold, call, hour, phone, tri, help, get, can, onlin, reserv, \n",
      "    topic 3 (level=1, total_words=2139, documents=417): email, can, number, pleas, book, dm, phone, confirm, not, form, \n",
      "    topic 4 (level=1, total_words=2081, documents=413): servic, custom, worst, ever, airlin, cancel, never, call, terribl, poor, \n",
      "    topic 5 (level=1, total_words=1938, documents=379): cancel, flight, flightl, hotel, delay, rebook, get, miss, airport, flightlat, \n",
      "    topic 6 (level=1, total_words=2183, documents=417): cancel, flight, hold, flightl, hour, get, late, wait, tomorrow, help, \n",
      "    topic 7 (level=1, total_words=667, documents=145): start, rout, love, see, av, geek, deal, new, newark, direct, \n",
      "    topic 8 (level=1, total_words=1776, documents=361): cancel, flight, flightl, tomorrow, dfw, due, book, weather, morn, problem, \n",
      "    topic 9 (level=1, total_words=726, documents=193): fli, love, blue, look, forward, alway, southwest, jet, happi, jetblu, \n",
      "    topic 10 (level=1, total_words=1249, documents=244): seat, class, first, upgrad, 1st, avail, paid, empti, plus, pay, \n",
      "    topic 11 (level=1, total_words=495, documents=125): bag, cloth, lost, luggag, wet, baggag, suitcas, clean, respons, deliveri, \n",
      "    topic 12 (level=1, total_words=1146, documents=263): thank, great, servic, custom, crew, awesom, amaz, happi, rock, guy, \n",
      "    topic 13 (level=1, total_words=946, documents=216): dragon, destin, ticket, show, love, pleas, see, best, vega, much, \n",
      "    topic 14 (level=1, total_words=430, documents=81): ceo, s, passeng, wall, battl, street, appeas, new, waterburi, republican, \n",
      "    topic 15 (level=1, total_words=1003, documents=229): worst, ever, airlin, never, fli, experi, best, unit, will, seen, \n",
      "    topic 16 (level=1, total_words=677, documents=187): thank, appreci, respons, quick, much, repli, second, prompt, respond, question, \n",
      "    topic 17 (level=1, total_words=572, documents=191): fleet, fleek, rt, stop, lol, oh, yall, k, lmao, jet, \n",
      "    topic 18 (level=1, total_words=342, documents=85): cross, finger, countri, food, eat, sunday, close, cut, dog, cup, \n",
      "    topic 19 (level=1, total_words=382, documents=82): airway, news, jblu, offer, rate, access, lufthansa, journal, short, corpor, \n",
      "    topic 20 (level=1, total_words=556, documents=121): snow, travel, tomorrow, advisori, winter, storm, weather, dfw, morn, ice, \n",
      "    topic 21 (level=1, total_words=528, documents=128): c, door, close, board, plane, b, seat, termin, minut, open, \n",
      "    topic 22 (level=1, total_words=1016, documents=285): dm, follow, sent, thank, can, pleas, send, info, tweet, messag, \n",
      "    topic 23 (level=1, total_words=1276, documents=270): mile, credit, card, ticket, book, mileag, account, point, use, chang, \n",
      "    topic 24 (level=1, total_words=365, documents=107): seat, clean, fi, rais, step, attend, leg, next, deserv, tabl, \n",
      "    topic 25 (level=1, total_words=585, documents=146): pm, flight, depart, arriv, suppos, leav, schedul, 8:00, tonight, departur, \n",
      "    topic 26 (level=1, total_words=513, documents=132): app, mobil, websit, site, work, passbook, updat, board, pass, iphon, \n",
      "    topic 27 (level=1, total_words=1202, documents=256): bag, check, luggag, baggag, claim, carri, lost, overhead, tag, put, \n",
      "    topic 28 (level=1, total_words=744, documents=163): plane, sit, gate, min, still, mechan, togeth, board, land, runway, \n",
      "    topic 29 (level=1, total_words=331, documents=82): reserv, add, old, child, wifi, food, see, log, meal, cabin, \n",
      "    topic 30 (level=1, total_words=257, documents=73): social, media, team, learn, rag, bereav, friend, process, disney, tmrw, \n",
      "    topic 31 (level=1, total_words=302, documents=79): earli, parti, bird, bought, oscar, 3rd, noth, enough, tix, sold, \n",
      "    topic 32 (level=1, total_words=586, documents=147): pass, board, companion, tsa, pre, number, show, reserv, add, print, \n",
      "    topic 33 (level=1, total_words=175, documents=65): cool, jump, ridicul, five, second, could'v, aww, departur, profit, gun, \n",
      "    topic 34 (level=1, total_words=546, documents=127): wifi, tv, watch, oscar, work, free, paid, jetblu, world, black, \n",
      "    topic 35 (level=1, total_words=336, documents=93): pay, attend, attitud, appl, travel, american, accept, exec, toward, plat, \n",
      "    topic 36 (level=1, total_words=230, documents=71): mark, thing, site, live, fair, pts, 15th, line, state, believ, \n",
      "    topic 37 (level=1, total_words=113, documents=41): sleep, zero, fail, effort, entir, floor, priceless, despit, pr, yeah, \n",
      "    topic 38 (level=1, total_words=146, documents=52): complain, much, well, 1k, hous, meal, e, sprint, texa, forc, \n",
      "    topic 39 (level=1, total_words=360, documents=111): san, diego, club, juan, francisco, chicago, admir, antonio, virgin, fli, \n",
      "    topic 40 (level=1, total_words=344, documents=85): car, seat, rental, rent, infant, bring, dfw, child, avail, drive, \n",
      "    topic 41 (level=1, total_words=257, documents=74): american, airlin, view, bad, alway, princess, half, food, serious, feb, \n",
      "    topic 42 (level=1, total_words=530, documents=123): late, flightr, call, hrs, back, seat, lost, still, worri, chang, \n",
      "    topic 43 (level=1, total_words=121, documents=49): costa, rica, friend, lisa, j, done, conveni, inconveni, kphl, everyon, \n",
      "    topic 44 (level=1, total_words=565, documents=142): call, number, baggag, claim, bag, phone, center, hung, spoke, east, \n",
      "    topic 45 (level=1, total_words=203, documents=60): name, mother, natur, sky, account, famili, unfortun, winter, marri, effort, \n",
      "    topic 46 (level=1, total_words=221, documents=69): st, cold, warm, place, loui, jet, degre, bos, probabl, stay, \n",
      "    topic 47 (level=1, total_words=162, documents=50): almost, million, deni, mile, miler, arriv, minor, chicago, fantast, embarrass, \n",
      "    topic 48 (level=1, total_words=310, documents=84): flyer, frequent, unhappi, usair, rude, stay, unhelp, loyal, gold, aa, \n",
      "    topic 49 (level=1, total_words=308, documents=87): communic, lack, rout, partner, res, platinum, concern, en, ana, respons, \n",
      "    topic 50 (level=1, total_words=208, documents=57): premier, bring, loyal, 1k, salt, swa, inflight, a-list, peanut, year, \n",
      "    topic 51 (level=1, total_words=275, documents=76): cancel, flightd, relat, weather, close, flightat, custom, behind, flightlat, depart, \n",
      "    topic 52 (level=1, total_words=245, documents=84): red, carpet, understand, rock, stori, im, treatment, deserv, twitter, shout, \n",
      "    topic 53 (level=1, total_words=239, documents=71): serv, drink, action, drunk, water, bottl, wheelchair, singl, snack, word, \n",
      "    topic 54 (level=1, total_words=687, documents=154): line, servic, custom, rep, told, poor, person, cust, counter, ppl, \n",
      "    topic 55 (level=1, total_words=263, documents=81): power, outlet, a320, domest, intern, love, road, appli, s, coffe, \n",
      "    topic 56 (level=1, total_words=357, documents=96): club, pass, loung, card, admir, access, give, free, member, certif, \n",
      "    topic 57 (level=1, total_words=297, documents=85): togeth, shit, get, post, act, real, captain, fire, sit, platinum, \n",
      "    topic 58 (level=1, total_words=213, documents=69): sky, unfriend, friend, south, agent, lol, team, flown, bird, funni, \n",
      "    topic 59 (level=1, total_words=203, documents=67): check, flt, especi, across, print, send, add, tweet, amaz, notif, \n",
      "    topic 60 (level=1, total_words=358, documents=89): fli, old, year, kid, jetblu, forward, help, sit, alon, suppos, \n",
      "    topic 61 (level=1, total_words=316, documents=76): system, comput, entertain, crash, much, broken, delta, paper, knew, children, \n",
      "    topic 62 (level=1, total_words=161, documents=56): profit, measur, weight, destroy, none, fit, billion, hung, other, plus, \n",
      "    topic 63 (level=1, total_words=162, documents=53): jet, arriv, blue, fraud, alert, clean, world, david, patient, plain, \n",
      "    topic 64 (level=1, total_words=136, documents=54): hate, phone, delet, peopl, europ, bullshit, unaccept, often, till, yea, \n",
      "    topic 65 (level=1, total_words=295, documents=82): love, brand, t, puerto, don, luv, song, w, rico, friday, \n",
      "    topic 66 (level=1, total_words=256, documents=86): west, drop, ball, beach, palm, absolut, head, rock, aa, key, \n",
      "    topic 67 (level=1, total_words=157, documents=52): spring, palm, beat, hell, lax, lesson, music, sent, ride, friday, \n",
      "    topic 68 (level=1, total_words=187, documents=62): seat, yes, liter, els, confirm, dr, manag, complain, appear, clt, \n",
      "    topic 69 (level=1, total_words=253, documents=77): seat, exit, row, huge, togeth, emerg, middl, orlando, mom, block, \n",
      "    topic 70 (level=1, total_words=145, documents=59): kudo, special, crew, cousin, wish, chief, rememb, honor, captain, 3hrs, \n",
      "    topic 71 (level=1, total_words=233, documents=68): exist, non, corpor, custom, stuck, suck, svc, orlean, fair, logo, \n",
      "    topic 72 (level=1, total_words=539, documents=135): email, complaint, file, respons, respond, claim, long, never, take, submit, \n",
      "    topic 73 (level=1, total_words=115, documents=48): wow, enough, laugh, n, sometim, direct, charact, perk, altern, den, \n",
      "    topic 74 (level=1, total_words=227, documents=75): hundr, sever, share, dollar, compani, figur, light, jfk, especi, shitti, \n",
      "    topic 75 (level=1, total_words=112, documents=45): deal, voucher, advis, bump, ran, convers, statement, cool, poor, awesom, \n",
      "    topic 76 (level=1, total_words=160, documents=58): english, speak, overnight, spanish, eventu, explain, quick, btw, win, husband, \n",
      "    topic 77 (level=1, total_words=212, documents=74): situat, handl, train, well, just, team, stress, ok, funni, funer, \n",
      "    topic 78 (level=1, total_words=194, documents=61): thought, seem, prob, other, injuri, insult, deserv, enough, add, behind, \n",
      "    topic 79 (level=1, total_words=250, documents=80): las, jfk, vega, afternoon, jet, passeng, york, lga, heathrow, bus, \n",
      "    topic 80 (level=1, total_words=293, documents=84): human, speak, asap, contact, dept, watch, fam, spent, mths, relat, \n",
      "    topic 81 (level=1, total_words=195, documents=65): old, lauren, im, real, la, tag, ice, might, believ, sent, \n",
      "    topic 82 (level=1, total_words=139, documents=55): except, insan, offer, failur, est, chanc, la, unlik, 2-Feb, phone, \n",
      "    topic 83 (level=1, total_words=181, documents=60): fail, epic, save, dalla, driven, elit, failur, welcom, guest, entir, \n",
      "    topic 84 (level=1, total_words=136, documents=50): suck, evalu, train, pos, need, cho, manner, attend, realiz, gear, \n",
      "    topic 85 (level=1, total_words=166, documents=66): drink, futur, coupon, free, spent, transport, card, ruin, 40min, beach, \n",
      "    topic 86 (level=1, total_words=270, documents=82): layov, high, welcom, mile, sure, club, across, request, airplan, paper, \n",
      "    topic 87 (level=1, total_words=145, documents=56): south, exampl, bird, assist, jeff, smisek, embarrass, america, choos, loyal, \n",
      "    topic 88 (level=1, total_words=150, documents=58): made, funer, lock, rough, miss, bet, lot, consider, roundtrip, narrowli, \n",
      "    topic 89 (level=1, total_words=195, documents=71): rout, wifi, base, provid, shuttl, fuel, law, concours, bring, china, \n",
      "    topic 90 (level=1, total_words=164, documents=57): week, status, match, fare, turn, met, us, six, rememb, london, \n",
      "    topic 91 (level=1, total_words=208, documents=67): water, hate, hard, slow, web, realli, will, password, taxi, confus, \n",
      "    topic 92 (level=1, total_words=91, documents=39): chi, option, 2-Jan, begin, period, ny, earlier, flightst, cheer, vet, \n",
      "    topic 93 (level=1, total_words=175, documents=68): name, nonstop, initi, sjc, dm'd, worri, mad, whether, broken, confus, \n",
      "    topic 94 (level=1, total_words=136, documents=56): dos, ku, handl, deserv, mood, light, baggag, spring, speak, wont, \n",
      "    topic 95 (level=1, total_words=253, documents=77): trip, round, date, march, price, fare, bought, lie, specif, around, \n",
      "    topic 96 (level=1, total_words=189, documents=69): haha, blow, fall, r, chat, peanut, jvm, constant, joke, cut, \n",
      "    topic 97 (level=1, total_words=142, documents=58): aircraft, possibl, els, miss, southwest, save, prepar, situat, rout, thank, \n",
      "    topic 98 (level=1, total_words=217, documents=70): cloth, friend, code, autom, wear, onboard, son, frustrat, comfort, sunshin, \n",
      "    topic 99 (level=1, total_words=206, documents=61): sky, peopl, four, break, flightl, 1k, truli, past, dfw, unreal, \n",
      "    topic 100 (level=1, total_words=152, documents=54): bday, san, almost, buddi, yay, golf, fran, 40th, sju, fulli, \n",
      "    topic 101 (level=1, total_words=182, documents=57): expir, unus, date, extens, fund, safeti, disconnect, not, see, wks, \n",
      "    topic 102 (level=1, total_words=194, documents=66): fare, polici, swa, low, guarante, denver, set, consist, buffalo, hello, \n",
      "    topic 103 (level=1, total_words=146, documents=48): safeti, equip, alway, share, flyer, first, announc, head, husband, ride, \n",
      "    topic 104 (level=1, total_words=141, documents=47): short, tarmac, staf, figur, state, colleg, public, wink, pa, delay, \n",
      "    topic 105 (level=1, total_words=153, documents=55): exact, kindl, w, thnx, dumb, def, kid, must, secur, tire, \n",
      "    topic 106 (level=1, total_words=174, documents=66): termin, case, front, ground, parti, feedback, submit, saturday, 13th, annual, \n",
      "    topic 107 (level=1, total_words=180, documents=56): airfar, leav, ticket, america, cheaper, match, virgin, luv, probabl, phl, \n",
      "    topic 108 (level=1, total_words=198, documents=63): prioriti, e-mail, letter, error, death, read, attitud, perhap, famili, treatment, \n",
      "    topic 109 (level=1, total_words=144, documents=60): runway, jan, high, disrespect, safe, request, luck, chariti, age, yet, \n",
      "    topic 110 (level=1, total_words=153, documents=53): lax, mission, mia, accomplish, mess, a1, held, pvd, market, dal, \n",
      "    topic 111 (level=1, total_words=156, documents=51): babi, not, quit, cri, let, fell, asleep, shock, wait, becom, \n",
      "    topic 112 (level=1, total_words=210, documents=58): buf, wheel, roc, ill, report, overnight, held, citi, captain, fll, \n",
      "    topic 113 (level=1, total_words=130, documents=51): next, ur, depart, choos, purchas, proceed, perfect, ireland, yell, jfk, \n",
      "    topic 114 (level=1, total_words=168, documents=57): price, week, wifi, page, load, onboard, rate, messag, transatlant, almost, \n",
      "    topic 115 (level=1, total_words=143, documents=55): sorri, weather, midway, incompet, uncomfort, ladi, cana, ty, understaf, summer, \n",
      "    topic 116 (level=1, total_words=169, documents=58): direct, practic, famili, experi, usair, 3yr, luggag, worri, detail, forward, \n",
      "    topic 117 (level=1, total_words=148, documents=54): c, e-mail, fit, misplac, nice, lost, profession, equip, dog, kc, \n",
      "    topic 118 (level=1, total_words=146, documents=54): fault, cours, attitud, yup, busi, work, us3825, harder, standard, lucki, \n",
      "    topic 119 (level=1, total_words=149, documents=46): coach, anymor, cabin, wa, spagnuolo, folk, privileg, switch, reimburs, check, \n",
      "    topic 120 (level=1, total_words=151, documents=62): thank, fill, land, begin, touch, survey, segment, local, p, smooth, \n",
      "    topic 121 (level=1, total_words=153, documents=55): date, differ, issu, manag, fall, apart, guess, switch, catch, tactic, \n",
      "    topic 122 (level=1, total_words=102, documents=46): final, bogota, area, pay, fill, access, set, dm'd, ruin, wallet, \n",
      "    topic 123 (level=1, total_words=196, documents=65): meal, parti, forgot, rest, disgust, order, correct, wow, rdu, mr, \n",
      "    topic 124 (level=1, total_words=108, documents=41): listen, aviv, tel, embassi, third, market, herndon, suit, drive, import, \n",
      "    topic 125 (level=1, total_words=150, documents=52): man, sale, goal, airplan, dream, aircraft, stt, posit, fa, friday, \n",
      "    topic 126 (level=1, total_words=129, documents=57): hate, refer, much, develop, bc, stupid, continent, funni, delay, get, \n",
      "    topic 127 (level=1, total_words=138, documents=52): la, intl, sister, brilliant, sju, em, check, bummer, part, florida, \n",
      "    topic 128 (level=1, total_words=160, documents=57): 24-Feb, februari, sent, miami, strand, y, thought, world, far, team, \n",
      "    topic 129 (level=1, total_words=141, documents=53): grand, stop, f, slc, major, reimburs, las, code, silicon, ul, \n",
      "    topic 130 (level=1, total_words=199, documents=61): tho, select, ski, offer, control, globe, angri, read, felt, everi, \n",
      "    topic 131 (level=1, total_words=97, documents=51): game, realiz, pittsburgh, failur, equip, storm, fresno, may, lost, comput, \n",
      "    topic 132 (level=1, total_words=173, documents=57): rout, goe, jacket, ground, sf, carryon, phoenix, shift, famili, simpl, \n",
      "    topic 133 (level=1, total_words=160, documents=56): tire, multipl, unexpect, pt, notif, 2a, dinner, headphon, stndbi, y, \n",
      "    topic 134 (level=1, total_words=152, documents=61): much, carrier, allianc, student, easi, school, actual, 10am, select, lynn, \n",
      "    topic 135 (level=1, total_words=130, documents=49): crappi, 1hr, see, centuri, 1st, consol, secur, quick, fabul, improv, \n",
      "    topic 136 (level=1, total_words=138, documents=50): surpris, upgrad, win, reimburs, pleasant, x, magazin, legroom, ba, compar, \n",
      "    topic 137 (level=1, total_words=39, documents=25): howev, site, purpos, staf, tank, sju, insid, iah, portland, honeymoon, \n",
      "    topic 138 (level=1, total_words=148, documents=60): broken, miami, took, tv, aircraft, overnight, e, remind, un, system, \n",
      "    topic 139 (level=1, total_words=100, documents=40): regular, clear, worthless, eat, dunno, list, white, suppos, profit, hous, \n",
      "    topic 140 (level=1, total_words=95, documents=43): thought, quit, kid, birthday, opportun, purchas, fianc, celebr, eventu, eyw, \n",
      "    topic 141 (level=1, total_words=132, documents=59): lose, answer, sever, elit, silver, lesson, last, fl, empti, abq, \n",
      "    topic 142 (level=1, total_words=165, documents=54): food, ad, call, comput, wink, knew, columbus, reschedul, inappropri, mia, \n",
      "    topic 143 (level=1, total_words=189, documents=59): pictur, price, right, partner, wed, mechan, voucher, flight, tail, share, \n",
      "    topic 144 (level=1, total_words=0, documents=3): zurich, endless, empti, emv, en, encount, encourag, end, energi, enrol, \n",
      "\n",
      ".."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5efe5cb18750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHierarchicalLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_levels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_levels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/apple/anaconda/lib/python2.7/site-packages/hlda/sampler.pyc\u001b[0m in \u001b[0;36mestimate\u001b[0;34m(self, num_samples, display_topics, n_words, with_weights)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python2.7/site-packages/hlda/sampler.pyc\u001b[0m in \u001b[0;36msample_path\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlevel_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_words\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_doc_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_word_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python2.7/site-packages/hlda/sampler.pyc\u001b[0m in \u001b[0;36mcalculate_doc_likelihood\u001b[0;34m(self, node_weights, level_word_counts)\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_word_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_word_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_topic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_word_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_word_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_topic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python2.7/site-packages/hlda/sampler.pyc\u001b[0m in \u001b[0;36mcalculate_word_likelihood\u001b[0;34m(self, node_weights, node, weight, level_word_counts, new_topic_weights, level)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             self.calculate_word_likelihood(node_weights, child, weight + node_weight,\n\u001b[0;32m--> 325\u001b[0;31m                                            level_word_counts, new_topic_weights, level+1)\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# finally if this is an internal node, add the weight of a new path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python2.7/site-packages/hlda/sampler.pyc\u001b[0m in \u001b[0;36mcalculate_word_likelihood\u001b[0;34m(self, node_weights, node, weight, level_word_counts, new_topic_weights, level)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_word_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_word_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_topic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_word_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_word_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_topic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# first calculate the likelihood of the words at this level, given this topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hlda = HierarchicalLDA(new_corpus, validwords, alpha=alpha, gamma=gamma, eta=eta, num_levels=num_levels)\n",
    "hlda.estimate(n_samples, display_topics=display_topics, n_words=n_words, with_weights=with_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14576"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_res = hlda.document_leaves\n",
    "doc_topic_res[0]\n",
    "all_group = []\n",
    "size_check = 0\n",
    "for n in range(145):\n",
    "    each_group = [ i for i in range(14640) if doc_topic_res[i].node_id == n]\n",
    "    size_check += len(each_group)\n",
    "    all_group.append(each_group)\n",
    "size_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node=146 level=1 customers=17 total_words=42 parent=0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_res[2265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_set = range(14640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_set = [i for group in all_group for i in group]\n",
    "all_group.append(list(set(total_set) - set(topic_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_groups = all_group[1:]\n",
    "## get the true labels here \n",
    "len(new_groups)\n",
    "#len(doc_topic_res)\n",
    "# True Lables \n",
    "#new_groups[142]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "0.660245901639\n"
     ]
    }
   ],
   "source": [
    "Origin_data = pd.read_csv(\"Tweets.csv\")\n",
    "True_label = Origin_data[\"airline_sentiment\"]\n",
    "\n",
    "# get the label for each group \n",
    "label_array = []\n",
    "correct_num = 0\n",
    "for groups in new_groups:\n",
    "    pos_sum = sum([ 1 for value in True_label[groups].values if value == \"positive\"] )\n",
    "    neu_sum = sum([ 1 for value in True_label[groups].values if value == \"neutral\"] )\n",
    "    neg_sum = sum([ 1 for value in True_label[groups].values if value == \"negative\"] )\n",
    "    temp_sum = [pos_sum,neu_sum,neg_sum]\n",
    "    if pos_sum == max(temp_sum):\n",
    "        label_array.append(\"positive\")\n",
    "    if neu_sum == max(temp_sum):\n",
    "        label_array.append(\"neutral\")\n",
    "    if neg_sum == max(temp_sum):\n",
    "        label_array.append(\"negative\")\n",
    "    correct_num += max(temp_sum)\n",
    "print(len(label_array))\n",
    "print(float(correct_num) / 14640 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### get the matrix result for the hlda\n",
    "file_matrix = []\n",
    "for n in range(len(new_groups)):\n",
    "    for group in new_groups[n]:\n",
    "        file_matrix.append([group,label_array[n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_label = pd.DataFrame(file_matrix)\n",
    "file_label.shape\n",
    "#tempdf = file_label.sort_values(by=[0])\n",
    "#tempdf.to_csv(\"hlda_result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
