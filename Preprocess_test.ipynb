{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading enwik9 - 1grams ...\n",
      "Reading enwik9 - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import string\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "seg = Segmenter(\"enwik9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#-:': '<seallips>',\n",
       " '#:': '<seallips>',\n",
       " \"(':\": '<happy>',\n",
       " '(((((((:': '<happy>',\n",
       " '((((((-:': '<happy>',\n",
       " '((((((:': '<happy>',\n",
       " '(((((-:': '<happy>',\n",
       " '(((((:': '<happy>',\n",
       " '((((-:': '<happy>',\n",
       " '((((:': '<happy>',\n",
       " '(((-:': '<happy>',\n",
       " '(((:': '<happy>',\n",
       " '((-:': '<happy>',\n",
       " '((:': '<happy>',\n",
       " \"(-':\": '<happy>',\n",
       " '(-:': '<happy>',\n",
       " '(-:0': '<angel>',\n",
       " '(-:<': '<devil>',\n",
       " '(-:O': '<angel>',\n",
       " '(-:o': '<angel>',\n",
       " '(-:{': '<devil>',\n",
       " '(:': '<happy>',\n",
       " '(:0': '<angel>',\n",
       " '(:<': '<devil>',\n",
       " '(:{': '<devil>',\n",
       " '(=': '<happy>',\n",
       " '(>_<)': '<sad>',\n",
       " '(o:': '<happy>',\n",
       " \")':\": '<sad>',\n",
       " ')))))))):': '<sad>',\n",
       " ')))))))-:': '<sad>',\n",
       " '))))))):': '<sad>',\n",
       " '))))))-:': '<sad>',\n",
       " ')))))):': '<sad>',\n",
       " ')))))-:': '<sad>',\n",
       " '))))):': '<sad>',\n",
       " '))))-:': '<sad>',\n",
       " ')))):': '<sad>',\n",
       " ')))-:': '<sad>',\n",
       " '))):': '<sad>',\n",
       " '))-:': '<sad>',\n",
       " ')):': '<sad>',\n",
       " \")-':\": '<sad>',\n",
       " ')-:': '<sad>',\n",
       " '):': '<sad>',\n",
       " '*)': '<wink>',\n",
       " '*-)': '<wink>',\n",
       " '*-:': '<kiss>',\n",
       " '*:': '<kiss>',\n",
       " '*\\\\0/*': '<happy>',\n",
       " '.-:': '<annoyed>',\n",
       " '0:)': '<angel>',\n",
       " '0:-)': '<angel>',\n",
       " '0:-3': '<angel>',\n",
       " '0:3': '<angel>',\n",
       " '0;^)': '<angel>',\n",
       " '3:)': '<devil>',\n",
       " '3:-)': '<devil>',\n",
       " '8)': '<happy>',\n",
       " '8-0': '<surprise>',\n",
       " '8-D': '<laugh>',\n",
       " '8-d': '<laugh>',\n",
       " '8D': '<laugh>',\n",
       " '8d': '<laugh>',\n",
       " ':#': '<seallips>',\n",
       " \":'(\": '<sad>',\n",
       " \":')\": '<happy>',\n",
       " \":'-(\": '<sad>',\n",
       " \":'-)\": '<happy>',\n",
       " ':(': '<sad>',\n",
       " ':((': '<sad>',\n",
       " ':(((': '<sad>',\n",
       " ':((((': '<sad>',\n",
       " ':(((((': '<sad>',\n",
       " ':((((((': '<sad>',\n",
       " ':(((((((': '<sad>',\n",
       " ':((((((((': '<sad>',\n",
       " ':)': '<happy>',\n",
       " ':))': '<happy>',\n",
       " ':)))': '<happy>',\n",
       " ':))))': '<happy>',\n",
       " ':)))))': '<happy>',\n",
       " ':))))))': '<happy>',\n",
       " ':)))))))': '<happy>',\n",
       " ':*': '<kiss>',\n",
       " ':-#': '<seallips>',\n",
       " ':-&': '<tong>',\n",
       " ':-(': '<sad>',\n",
       " ':-((': '<sad>',\n",
       " ':-(((': '<sad>',\n",
       " ':-((((': '<sad>',\n",
       " ':-(((((': '<sad>',\n",
       " ':-((((((': '<sad>',\n",
       " ':-(((((((': '<sad>',\n",
       " ':-)': '<happy>',\n",
       " ':-))': '<happy>',\n",
       " ':-)))': '<happy>',\n",
       " ':-))))': '<happy>',\n",
       " ':-)))))': '<happy>',\n",
       " ':-))))))': '<happy>',\n",
       " ':-*': '<kiss>',\n",
       " ':-,': '<wink>',\n",
       " ':-.': '<annoyed>',\n",
       " ':-/': '<annoyed>',\n",
       " ':-<': '<sad>',\n",
       " ':-D': '<laugh>',\n",
       " ':-O': '<surprise>',\n",
       " ':-P': '<tong>',\n",
       " ':-X': '<seallips>',\n",
       " ':-[': '<sad>',\n",
       " ':-b': '<tong>',\n",
       " ':-c': '<sad>',\n",
       " ':-d': '<laugh>',\n",
       " ':-o': '<surprise>',\n",
       " ':-p': '<tong>',\n",
       " ':-x': '<seallips>',\n",
       " ':-|': '<annoyed>',\n",
       " ':-||': '<sad>',\n",
       " ':-Þ': '<tong>',\n",
       " ':-þ': '<tong>',\n",
       " ':/': '<annoyed>',\n",
       " ':3': '<happy>',\n",
       " ':<': '<sad>',\n",
       " ':>': '<happy>',\n",
       " ':@': '<sad>',\n",
       " ':D': '<laugh>',\n",
       " ':L': '<annoyed>',\n",
       " ':O': '<surprise>',\n",
       " ':P': '<tong>',\n",
       " ':S': '<annoyed>',\n",
       " ':X': '<seallips>',\n",
       " ':[': '<sad>',\n",
       " ':\\\\': '<annoyed>',\n",
       " ':]': '<happy>',\n",
       " ':^)': '<happy>',\n",
       " ':b': '<tong>',\n",
       " ':c': '<sad>',\n",
       " ':c)': '<happy>',\n",
       " ':d': '<laugh>',\n",
       " ':l': '<annoyed>',\n",
       " ':o': '<surprise>',\n",
       " ':o)': '<happy>',\n",
       " ':p': '<tong>',\n",
       " ':s': '<annoyed>',\n",
       " ':x': '<kiss>',\n",
       " ':{': '<sad>',\n",
       " ':|': '<sad>',\n",
       " ':}': '<happy>',\n",
       " ':Þ': '<tong>',\n",
       " ':þ': '<tong>',\n",
       " ';)': '<wink>',\n",
       " ';-)': '<wink>',\n",
       " ';-]': '<wink>',\n",
       " ';D': '<wink>',\n",
       " ';]': '<wink>',\n",
       " ';^)': '<wink>',\n",
       " ';d': '<wink>',\n",
       " '<3': '<heart>',\n",
       " '<:': '<happy>',\n",
       " '<:-|': '<annoyed>',\n",
       " '=)': '<happy>',\n",
       " '=-3': '<laugh>',\n",
       " '=-D': '<laugh>',\n",
       " '=-d': '<laugh>',\n",
       " '=/': '<annoyed>',\n",
       " '=3': '<laugh>',\n",
       " '=D': '<laugh>',\n",
       " '=L': '<annoyed>',\n",
       " '=\\\\': '<annoyed>',\n",
       " '=]': '<happy>',\n",
       " '=d': '<laugh>',\n",
       " '=l': '<annoyed>',\n",
       " '=p': '<tong>',\n",
       " '>-:': '<sad>',\n",
       " '>.<': '<annoyed>',\n",
       " '>:': '<sad>',\n",
       " '>:)': '<devil>',\n",
       " '>:-)': '<devil>',\n",
       " '>:-D': '<devil>',\n",
       " '>:-d': '<devil>',\n",
       " '>:/': '<annoyed>',\n",
       " '>:D': '<devil>',\n",
       " '>:O': '<surprise>',\n",
       " '>:P': '<tong>',\n",
       " '>:[': '<sad>',\n",
       " '>:\\\\': '<annoyed>',\n",
       " '>:d': '<devil>',\n",
       " '>:o': '<surprise>',\n",
       " '>:p': '<tong>',\n",
       " '>;)': '<devil>',\n",
       " '>_>^': '<highfive>',\n",
       " '@:': '<sad>',\n",
       " 'B^D': '<laugh>',\n",
       " \"D-':\": '<sad>',\n",
       " 'D8': '<sad>',\n",
       " 'D:': '<sad>',\n",
       " 'D:<': '<sad>',\n",
       " 'D;': '<sad>',\n",
       " 'D=': '<sad>',\n",
       " 'DX': '<sad>',\n",
       " 'O-:': '<surprise>',\n",
       " 'O:': '<surprise>',\n",
       " 'O:-)': '<angel>',\n",
       " 'O:<': '<surprise>',\n",
       " 'X-D': '<laugh>',\n",
       " 'X-P': '<tong>',\n",
       " 'XD': '<laugh>',\n",
       " 'XP': '<tong>',\n",
       " '[:': '<happy>',\n",
       " '[=': '<happy>',\n",
       " '\\\\o/': '<happy>',\n",
       " ']-:': '<sad>',\n",
       " ']:': '<sad>',\n",
       " ']:<': '<sad>',\n",
       " '^5': '<highfive>',\n",
       " '^<_<': '<highfive>',\n",
       " 'b^d': '<laugh>',\n",
       " \"d-':\": '<sad>',\n",
       " 'd8': '<sad>',\n",
       " 'd:': '<sad>',\n",
       " 'd:<': '<sad>',\n",
       " 'd;': '<sad>',\n",
       " 'd=': '<sad>',\n",
       " 'dx': '<sad>',\n",
       " 'o-:': '<surprise>',\n",
       " 'o-o': '<surprise>',\n",
       " 'o.O': '<surprise>',\n",
       " 'o.o': '<surprise>',\n",
       " 'o/\\\\o': '<highfive>',\n",
       " 'o:': '<surprise>',\n",
       " 'o:-)': '<angel>',\n",
       " 'o:<': '<surprise>',\n",
       " 'o_0': '<surprise>',\n",
       " 'o_O': '<surprise>',\n",
       " 'o_o': '<surprise>',\n",
       " 'v.v': '<sad>',\n",
       " 'x-D': '<laugh>',\n",
       " 'x-d': '<laugh>',\n",
       " 'x-p': '<tong>',\n",
       " 'x:': '<kiss>',\n",
       " 'xD': '<laugh>',\n",
       " 'xd': '<laugh>',\n",
       " 'xp': '<tong>',\n",
       " '{:': '<happy>',\n",
       " '|-:': '<annoyed>',\n",
       " '|-:>': '<annoyed>',\n",
       " '|-O': '<surprise>',\n",
       " '|-o': '<surprise>',\n",
       " '|:': '<sad>',\n",
       " '|;-)': '<happy>',\n",
       " '||-:': '<sad>',\n",
       " '}:': '<sad>',\n",
       " '}:)': '<devil>',\n",
       " '}:-)': '<devil>',\n",
       " '°o°': '<surprise>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the emoticons to transform symbol to emotion\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading enwik9 - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "## this corrector is not working quite well. so decide no to use at all. \n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "sp = SpellCorrector(corpus=\"enwik9\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guy\n"
     ]
    }
   ],
   "source": [
    "print(sp.correct(\"guuuy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Comment\n",
    "- removed special characters e.g. emoticons\n",
    "- removed name tags starting from @, including airline names\n",
    "- remove line space (\\n)\n",
    "\n",
    "## Decide \n",
    "- remove tags with #?\n",
    "- remove numbers?\n",
    "- remove website? (after http)\n",
    "\n",
    "\n",
    "## ToDo \n",
    "- not : is removed by stop_words\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoding=utf8  \n",
    "import sys  \n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Tweets.csv')\n",
    "doc_set = data['text']\n",
    "doc_set = doc_set.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "## https://stackoverflow.com/questions/42370508/how-to-delete-special-characters-such-as-%C5%92%C3%B0%C5%B8-from-tweets\n",
    "def AsciiFilter(var):\n",
    "    temp = ''.join([c for c in var if ord(c) < 128])\n",
    "    return str(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc_set2 = [text.encode(errors='ignore').decode('utf-8') for text in doc_set]\n",
    "doc_set = [AsciiFilter(text) for text in doc_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "# from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "# tokenizer = SocialTokenizer()\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "#p_stemmer = PorterStemmer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " \"how's\",\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"let's\",\n",
       " 'me',\n",
       " 'more',\n",
       " 'most',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'same',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'very',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'when',\n",
       " \"when's\",\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whom',\n",
       " 'why',\n",
       " \"why's\",\n",
       " 'with',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@virginamerica What is your phone number. I can't find who to call about a flight reservation.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_set[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdoc = doc_set[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negs = [w for w in en_stop if \"n't\" in w]\n",
    "negsInto = [\"are not\", \"can not\", \"could not\", \"did not\", \"does not\", \"do not\",\n",
    "           \"had not\", \"has not\", \"have not\", \"is not\", \"must not\", \"shall not\",\n",
    "           \"should not\", \"was not\", \"were not\", \"will not\", \"would not\"]\n",
    "negDic = {}\n",
    "for v,w in zip(negs, negsInto) :\n",
    "    negDic[v] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_stop.remove('not')\n",
    "en_stop.remove('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvertNeg(lowerdoc) :\n",
    "    doc = lowerdoc\n",
    "    for w in lowerdoc.split(' ') :\n",
    "        if w in negs :\n",
    "            doc = doc.replace(w,negDic[w])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DeleteLine(lowerdoc) :\n",
    "    doc = lowerdoc\n",
    "    if '\\n' in lowerdoc :\n",
    "        doc = doc.replace('\\n',' ')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NametagTokenFilter(tokens) :\n",
    "    temp = [str(word) for word in tokens if not word.startswith('@')]\n",
    "    return temp\n",
    "\n",
    "def WebAddressFilter(tokens) :\n",
    "    temp = [str(word) for word in tokens if not word.startswith(\"http\")]\n",
    "    return temp\n",
    "# def NonCharacterFilter(tokens) :\n",
    "#     temp = [ re.sub(\"[^a-zA-Z#]\", \"\",word) for word in tokens]\n",
    "#     return temp\n",
    "\n",
    "def NumberFilter(tokens) :\n",
    "    temp = [str(word) for word in tokens if not word.isdigit()]\n",
    "    return temp\n",
    "# def SpaceFilter(tokens) :\n",
    "#     temp = [str(word) for word in tokens if not str(word)==\"\"]\n",
    "#     return temp\n",
    "def HashRemover(tokens) :\n",
    "    temp = [seg.segment(i[1:]).split(\" \") if i.startswith('#') else [i] for i in tokens]\n",
    "    # flatten the list \n",
    "    temp = [item for sublist in temp for item in sublist]\n",
    "    return temp\n",
    "def TransEmotionFilter(tokens) :\n",
    "    temp = [emoticons[word][1:-1] if word in emoticons else word for word in tokens]\n",
    "    return temp\n",
    "## eliminate all doesn't contains either digits or letters \n",
    "def stringPuncFilter(tokens):\n",
    "    temp = [str(word) for word in tokens if bool(re.search(r'(\\d)|([a-z])',str(word)))]\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@virginamerica what is your phone number. i can not find who to call about a flight reservation.\n",
      "['@virginamerica', 'what', 'is', 'your', '#phonenumber', '.', 'i', 'can', 'not', 'find', 'who', 'to', 'call', 'about', 'a', '#flightreservation', '.', ':)', '...']\n",
      "['what', 'is', 'your', '#phonenumber', '.', 'i', 'can', 'not', 'find', 'who', 'to', 'call', 'about', 'a', '#flightreservation', '.', ':)', '...']\n"
     ]
    }
   ],
   "source": [
    "doctext = testdoc\n",
    "# clean and tokenize document string\n",
    "raw = doctext.lower()\n",
    "raw = ConvertNeg(raw)\n",
    "raw = DeleteLine(raw)\n",
    "print(raw)\n",
    "\n",
    "### test the spliter to see\n",
    "raw = \"@virginamerica what is your #phonenumber. i can not find who to call about a #flightreservation. :) ...\"\n",
    "tokens = tokenizer.tokenize(raw)\n",
    "print(tokens)\n",
    "tokens = NametagTokenFilter(tokens)\n",
    "tokens = WebAddressFilter(tokens)\n",
    "tokens = NumberFilter(tokens)\n",
    "# tokens = NonCharacterFilter(tokens)\n",
    "# tokens = SpaceFilter(tokens)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'your', 'phone', 'number', '.', 'i', 'can', 'not', 'find', 'who', 'to', 'call', 'about', 'a', 'flight', 'reservation', '.', ':)', '...']\n"
     ]
    }
   ],
   "source": [
    "tokens = HashRemover(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'your', 'phone', 'number', '.', 'i', 'can', 'not', 'find', 'who', 'to', 'call', 'about', 'a', 'flight', 'reservation', '.', 'happy', '...']\n"
     ]
    }
   ],
   "source": [
    "tokens = TransEmotionFilter(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phone', 'number', 'can', 'not', 'find', 'call', 'flight', 'reservation', 'happy']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words from tokens\n",
    "tokens = [i for i in tokens if not i in string.punctuation]\n",
    "tokens = stringPuncFilter(tokens)\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phone', 'number', 'can', 'not', 'find', 'call', 'flight', 'reserv', 'happi']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# stem tokens\n",
    "#stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "stemmed_tokens = [s_stemmer.stem(i) for i in stopped_tokens]\n",
    "print(stemmed_tokens)\n",
    "# add tokens to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in range(len(doc_set)):\n",
    "    doctext = doc_set[i]\n",
    "    # clean and tokenize document string\n",
    "    raw = doctext.lower()\n",
    "    raw = ConvertNeg(raw)\n",
    "    raw = DeleteLine(raw)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    tokens = NametagTokenFilter(tokens)\n",
    "    tokens = WebAddressFilter(tokens)\n",
    "    tokens = HashRemover(tokens)\n",
    "    tokens = NumberFilter(tokens)\n",
    "    tokens = TransEmotionFilter(tokens)\n",
    "    \n",
    "\n",
    "    # remove stop words from tokens\n",
    "    tokens = [i for i in tokens if not i in string.punctuation]\n",
    "    tokens = stringPuncFilter(tokens)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    #stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    stemmed_tokens = [s_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1123fd908>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "#data = [['tom','want','apple'],['tom','love','pear']]\n",
    "res_array = mlb.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9147"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_relook = mlb.classes_\n",
    "words_df = pd.DataFrame(words_relook)\n",
    "words_df.to_csv('words_dict.csv',index = False)\n",
    "len(words_relook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9147"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9147"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_relook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight\n",
      "cancel\n",
      "flightl\n",
      "delay\n",
      "hour\n",
      "get\n",
      "no\n",
      "now\n",
      "late\n",
      "book\n",
      "\n",
      "\n",
      "not\n",
      "can\n",
      "get\n",
      "help\n",
      "will\n",
      "bag\n",
      "call\n",
      "still\n",
      "time\n",
      "plane\n",
      "\n",
      "\n",
      "thank\n",
      "servic\n",
      "custom\n",
      "no\n",
      "just\n",
      "help\n",
      "hour\n",
      "airlin\n",
      "now\n",
      "call\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_words = [3555,1953,3569,2722,4260,3834,5706,5785,4858,1695,5769,1948,3834,4125,8928,1451,1933,7626,8030,6255,7940,7191,2579,5706,4719,4125,4260,1056,5785,1933]\n",
    "n = 1\n",
    "for i in topic_words:\n",
    "    print(words_relook[i-1]),\n",
    "    if n%10 == 0:\n",
    "        print(\"\\n\")\n",
    "    n = n+1\n",
    "## From the result, the 1st group is    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight\n",
      "not\n",
      "delay\n",
      "get\n",
      "can\n",
      "hour\n",
      "no\n",
      "late\n",
      "thank\n",
      "now\n",
      "\n",
      "\n",
      "not\n",
      "thank\n",
      "can\n",
      "get\n",
      "no\n",
      "servic\n",
      "custom\n",
      "just\n",
      "help\n",
      "will\n",
      "\n",
      "\n",
      "cancel\n",
      "flight\n",
      "flightl\n",
      "not\n",
      "get\n",
      "can\n",
      "no\n",
      "help\n",
      "hold\n",
      "hour\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in list([3554,5768,2721,3833,1947,4259,5705,4857,7939,5784]): \n",
    "    print(words_relook[i])\n",
    "print(\"\\n\")\n",
    "for i in [5768,7939,1947,3833,5705,7190,2578,4718,4124,8927]:\n",
    "    print(words_relook[i])\n",
    "print(\"\\n\")\n",
    "for i in [1952,3554,3568,5768,3833,1947,5705,4124,4190,4259]:\n",
    "    print(words_relook[i])\n",
    "print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res_array)\n",
    "df.to_csv('matrix_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(texts)\n",
    "df.to_csv('clean_data.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14639\n"
     ]
    }
   ],
   "source": [
    "with open(\"tweeters_snowball_ver2.txt\", \"a\") as f : \n",
    "    for i in range(len(texts)) :\n",
    "        docID = 'doc'+str(i)\n",
    "        words = ''\n",
    "        for w in texts[i] :\n",
    "            words = words + w + ' '\n",
    "        f.write(docID + ' ' + words)\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts[7095]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ppl',\n",
       " u'need',\n",
       " u'know',\n",
       " u'mani',\n",
       " u'seat',\n",
       " u'next',\n",
       " u'flight',\n",
       " u'plz',\n",
       " u'put',\n",
       " 'us',\n",
       " u'standbi',\n",
       " u'peopl',\n",
       " u'next',\n",
       " u'flight']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[len(texts)-1]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
