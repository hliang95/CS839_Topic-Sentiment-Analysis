# Sentimental analysis of the Airline Tweets Data
% Group Member: Huan Liang, Sujee Lee, Yaxin Deng
## Preprocessing
### Introduction 
The data pipeline starts with collecting the data and ends with communicating the results. 
We introduce the dataset first. In this project, we are using the twitter data on multiple American airline companies, including: American, Delta, Southwest, United, US Airways and Virgin America. The important dimensions we are using in this dataset are 'Text', this dimension contains all the plain text of the twitter, "Sentiment Label", this dimension contains 3 values - Negative, Neutral, Positive, "Airline", this dimension shows the airline company the twitter talks about, "NegativeReason", this dimension shows why user complains about the company, the reason including 'Bad Flight', 'Late Flight', 'Customer Service' and others. 
Data pre-processing includes cleaning, instance selection, normalization, transformation, feature extraction and selection, etc. In this project, we focus on the data cleaning in the whole preprocessing cycle. We do the cleaning job in the 'Text' dimension for the JST(Joint Sentiment/Topic model) usage. 
#### Background 
The twitter data - tweets have many unique features which is quite different from the document text appeared in many NLP projects. Tweets are short messages, restricted to 140 characters in length. This nature of the quick and short messages, people would use acronyms, use special characters that express emotional meanings. There are many main things we have found in the twitter text data. *emoticons*: using punctuation and letters to form the facial expression and to express the user's mood. e.g. :) means happy. and :-D mean laugh. *target*: the twitter user uses "@" symbol to mention others. In the dataset we have used, the 'Airline' dimension is generated by this action. *Hashtags*: This is a unique feature that appears commonly in the social media text. Users use # symbol and no space seperated texts to mark topics. *Attached websites*: Users sometimes post an url link to the websites in the twitter to direct others. In this project, we are not going to dig into what that website says. 
The dataset contains 14640 tweets. And all of them have valid words and information. 9178 of them are negative, 3099 of them are neutral, and 2363 of them are positive. This leaves us with an unbalanced sample of all the tweets. And for all 6 companies, we have American-2759,Delta-2222, Southwest-2420, United-3822,US Airways-2913,Virgin America-504. 
#### Packages and Steps
We have used many packages in the preprocessing routine. The main packages we have used are *nltk* and *ekphrasis*. We also used *pandas* to convert the data from csv to dataframe as a common approach to do data preprocessing. 
In order to make sure that the topic model is identifying interesting or important patterns instead of noise, we have done several things as the cleaning steps. 
  - First, we remove special characters (non-ascii characters) as there would be some 'weird' characters in the tweets when transforming them to the text form.
  - Second, we use the *nltk TweetTokenizer* in our data. It is different from the common tokenize as it understands complex emoticons, emojis and other unstructured expressions like dates, times and more.
``` python
>>> from nltk.tokenize import TweetTokenizer
>>> tknzr = TweetTokenizer()
>>> s0 = "This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--"
>>> tknzr.tokenize(s0)
['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']
```
  - Third, we remove the basic stop words, string punctuations, the '@' target and the attached websites. We do this by building the functions on our own. 
  - Fourth, we try to split the hashtag in the tweets using *word-segmentation*. The hashtag is the unique feature of the tweets. We are using the *ekphrasis* packages to do this. The implementation requires word statistics in order to identify and separating the words in a string. And we generated the statistical data using wikipedia. 
``` python
>>> from ekphrasis.classes.segmenter import Segmenter
>>> seg = Segmenter(corpus="enwik9") 
>>> print(seg.segment("smallandinsignificant"))
>>> 'small and insignificant'
```
  - Fifth, we convert all the emoticons to the word expression using a emoticon dictionary. The dictionary contain {':-(': 'sad', ':-)': 'happy',...}. And after converting, we eliminate all the nonsense punctionations such as '--->', '...', etc. 
  - At last, we use Snowball stemming algorithm fromm *nltk* package to eliminate affixes (suffixes, prefixes, infixes, circumfixes) from a word in order to obtain a word stem. For example, "running -> run". 

The 6 steps constructs our data preprocessing pipeline.At the end of all these cleaning steps, our resulting data is essentially composed of unique reasonable words. 
These cleaned data can then be transformed to either a plan text with document number or just a document-term matrix to assist all the unsupervised methods. Now it is ready to put our data into the topic models. 

## Methods Description and Results Comparison
We have run 4 unsupervised models in total on our data set. They are K-Means, matrix factorization using Alternating Least Square approach (ALS), Latent Dirichlet Allocation algorithm (LDA) and Joint Sentiment/Topic model(JST). 
The comparison result is shown as below. 
Accuracies are :!

| Model    | K-means | ALS | LDA | JST |
|:--------:|:-------:|:---:|:---:|:---:|
| Accuracy | 0.2338 | 0.3834 | 0.5507 | 0.4811 |

![image](./dist_true.png)

From the current result, Th eLDA working better 
### K-Means
K-Means clustering is an unsupervised learning method. We used this method as to classify our data set into 3 groups in total. And then extract the topic words in each cluster to get a clue on what the sentiment of each cluster is. 

| Topic 1 (positive) | Topic 2 (neutral) | Topic 3 (negative) |
|------------|------------|------------|
| flight | not | cancel |
| not | thank | flight |
| delay | can | flightl |
| get | get | not |
| can | no | get |
| hour | servic | can |
| no | custom | no |
| late | just | help |
| thank | help | hold |
| now | will | hour |

Distributions of sentiments from the models are

![image](./dist_kmeans.png)

![image](./kmeans_cm_1.png) ![image](./kmeans_cm_2.png)
### ALS
The Hierarchical Alternating Least Square approach in Nonnegative Matrix Factorization is commonly used in the topic recovery and document classification. And we implemented this topic using the Matlab code we have developed previously. 
The documents 

| Topic 1 (positive) | Topic 2 (neutral) | Topic 3 (negative) |
|------------|------------|------------|
| thank | not | flight |
| servic | can  | cancel |
| custom | get | flightl |
| no | help | delay |
| just | will | hour |
| help | bag | get |
| hour | call | no |
| airlin | still | now |
| now | time | late |
| call | plane | book |

From the topic words of the ALS, it is easy to see that Topic 1 works is describing the positive groups, as it has the representative words as 'Thank'. As for the negative cluster, some words can be easily classified into the negative sentiment group such as 'cancel', 'delay', 'late'. Based on this analysis, we assigned each group the corresponding sentimental label. 

![image](./dist_als.png)

![image](./als_cm_1.png) ![image](./als_cm_2.png)

The distribution of the result shows that the ALS tends to classify the tweet into positive groups even thougn we have more negative tweeets in the true distribution. 
The confusion matrix shows the high accuracy in the positive cluster, as high as 75%. But the neurtral and negative group performances are worse. They are all between 30% to 40%. This shows the bad classification performance in the ALS method. 
### LDA
The LDA model is such a popular algorithm in the unsupervised machine learning area that we can't miss. In the algorithm, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. 
We used topic models to classify the tweets into 3 groups. Here, we assume that each of sentiments, positive, neutral, and negative, can represent a topic, respectively. When training LDA model, we set the number of topics as 3.
The following table shows the most probable workds in each topic.

| Topic 1 (positive) | Topic 2 (neutral) | Topic 3 (negative) |
|------------|------------|------------|
| thank | not | flight |
| servic | fli | not |
| custom | airlin | can |
| great | flight | get |
| happi | will | no |
| respons | seat | cancel |
| yes | never | hour |
| just | time | call |
| appreci | us | help |
| rude | ever | delay |

From looking at the words, we can tell the first topic is more related to ‘positive’ sentiment. Next, the third topic is more like to be ‘negative’ sentiment, and the other one is ‘neutral’. Using those sentiment labels to the topics

![image](./dist_lda.png)

![image](./lda_cm_1.png) ![image](./lda_cm_2.png)
### JST 
![image](./dist_jst.png)

![image](./jst_cm_1.png) ![image](./jst_cm_2.png)

| Topic 1 () | Topic 2 () | Topic 3 () |
|------------|------------|------------|
| thank | not | flight |
| servic | fli | not |
| custom | airlin | can |
| great | flight | get |
| happi | will | no |
| respons | seat | cancel |
| yes | never | hour |
| just | time | call |
| appreci | us | help |
| rude | ever | delay |

## Next step
We have worked on the TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency. And we plan to implement the model shown in the paper and run it on our tweets data to see how it works. 
Currently, we have found several reference materials including both codes and analysis on GitHub. However, all of them is either in progress or not working at all. Most of the prerequsite packages implementation require the help from GPU. This is the main barrier when we trying to construct the model.  